{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目的：\n",
    "使用一站式在线平台搭建神经网络，实现深度学习算法的网络组建、训练、预测过程\n",
    "### 实验要求：\n",
    "完成给定数据集上的分类任务，定义全连接前馈神经网络，softmax输出层以及损失函数 ，训练网路，使用给定的数据集中的测试数据验证精度。\n",
    "### 基本的实验数据集\n",
    "MNIST数据集 Cifar10数据集等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 搭建神经网络\n",
    "（1）加载数据，引入相关的包\n",
    "\n",
    "（2）根据网络结构，定义各参数、变量，这是最重要的一步，搭建好网络结构就只需要喂数据\n",
    "\n",
    "（3）搭建好网络结构，提供数据进行训练并保存训练模型\n",
    "\n",
    "（4）用数据训练好模型\n",
    "\n",
    "（5）模型评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 搭建网络结构\n",
    "\n",
    "1. keras.Sequential() 函数创建顺序模型\n",
    "2. model.add函数来添加网络结构的层数，简单网络一般是输入层，隐含层，输出层三层结构\n",
    "3. keras.layers.Dense 函数创建全连接的网络，有两个关键的参数，一个是神经元的个数units,一个是激活函数的类型activation，注意:每一层神经元的个数是上一层的输出个数\n",
    "4. keras.layers.Conv2D 创建卷积层，过滤器filters,卷积核的大小kernel_size,边缘处理padding,激活函数activation\n",
    "5. keras.layers.MaxPool2D 创建完卷积层，然后就是用池化层，这里用最大池化和平均池化，pool_size池化的大小\n",
    "6. 最后还是一样，一个全连接的输出层\n",
    "* 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 搭建 简单 模型 \n",
    "def build_simple_model(out_dims, img_size):\n",
    "    inputs_dim = Input((img_size, img_size, 3))\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same',activation='relu')(inputs_dim)\n",
    "    x = Conv2D(filters=32, kernel_size=3, strides=1, padding='same',activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=1, padding='same',activation='relu')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=1, padding='same',activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    x_flat = Flatten()(x)\n",
    "    fc1 = Dense(512,activation='relu')(x_flat)\n",
    "    dp_1 = Dropout(0.4)(fc1)\n",
    "\n",
    "    fc2 = Dense(out_dims)(dp_1)\n",
    "    fc2 = Activation('softmax')(fc2)\n",
    "\n",
    "    model = Model(inputs=inputs_dim, outputs=fc2)\n",
    "    return model\n",
    "\n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f,encoding='latin1')\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "        X = X.reshape(10000, 3, 32,32).transpose(0,2,3,1).astype(\"float\") #channel last\n",
    "        Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "        X, Y = load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "    return Xtr, Ytr, Xte, Yte\n",
    "\n",
    "ROOT = '/kaggle/input/dataset-cifar/cifar-10-batches-py'\n",
    "X_train,y_train,X_test,y_test = load_CIFAR10(ROOT)\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "y_train = np_utils.to_categorical(y_train, num_classes = 10)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes = 10)\n",
    "\n",
    "X_train = X_train.astype(np.float32) #'float32'占4个字节\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.uint8) # 'uint8'占1个字节\n",
    "y_test = y_test.astype(np.uint8)\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=0)\n",
    "\n",
    "\n",
    "simple_model = build_simple_model(10,32)\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 网络结构搭建完后 ，开始训练模型\n",
    "1. 在这之前可以对数据做一些处理，正则化等操作这里用了标准化数据\n",
    "2. np.expand_dims 看成 增加维度，变化数据的维度\n",
    "3. 标签需要one_hot，使用onehot的直接原因是现在多分类cnn网络的输出通常是softmax层，而它的输出是一个概率分布，从而要求输入的标签也以概率分布的形式出现，进而算交叉熵之类。\n",
    "4. 配置训练模型。optimizer优化器，loss损失函数，学习率，迭代次数等参数\n",
    "* 最后开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelCheckpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9910959b7746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m checkpointer = ModelCheckpoint(filepath='cifa_10_simple.hdf5', \n\u001b[0m\u001b[1;32m      2\u001b[0m                               verbose=1, save_best_only=True) #保存最好模型权重\n\u001b[1;32m      3\u001b[0m simple_model.compile(optimizer='adam',\n\u001b[1;32m      4\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=['accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='cifa_10_simple.hdf5', \n",
    "                              verbose=1, save_best_only=True) #保存最好模型权重\n",
    "simple_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "history = simple_model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          epochs=epochs,callbacks=[checkpointer],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能评估 \n",
    "\n",
    "用开始创建的类history绘图的acc和loss变化情况，可以看每一层迭代时的变化情况\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**参数对性能的影响:**\n",
    "\n",
    "可参考 ：https://blog.csdn.net/program_developer/article/details/78597738\n",
    "\n",
    "这里是举几个参数说明:\n",
    "\n",
    "激活函数： \n",
    "\n",
    "Sigmoid函数：常用的非线性的激活函数，它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.\n",
    "\n",
    "tanh函数：解决了Sigmoid函数的不是zero-centered输出问题\n",
    "\n",
    "Relu函数：ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试\n",
    "\n",
    "迭代次数：epochs（epochs指的就是训练过程中数据将被“轮”多少次）\n",
    "\n",
    "训练的数量: Batch Size（batch size将决定我们一次训练的样本数目）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用ModelCheckpoint 记录中间训练的过程\n",
    "\n",
    "最后绘制 acc 和 loss 的变化情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-08ae00e16cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %matplotlib inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt                        \n",
    "# %matplotlib inline    \n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
