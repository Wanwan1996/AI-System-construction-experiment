{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print('python_version',platform.python_version())\n",
    "print('tensorflow',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mnist=keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        #创建一个图\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')#plt.plot(x,y)，这个将数据画成曲线\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)#设置网格形式\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')#给x，y轴加注释\n",
    "        plt.legend(loc=\"upper right\")#设置图例显示位置\n",
    "        plt.show()\n",
    "\n",
    "#创建一个实例LossHistory\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 数据 \n",
    "def get_train_val(mnist_path):\n",
    "    # mnist下载地址：https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data(mnist_path)\n",
    "    # 打印 训练数据和测试数据的长度\n",
    "    print(\"train_images nums:{}\".format(len(train_images)))\n",
    "    print(\"test_images nums:{}\".format(len(test_images)))\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    " \n",
    "# 展示 图片 数据\n",
    "def show_mnist(images,labels):\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([ ])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(images[i],cmap=plt.cm.gray)\n",
    "        plt.xlabel(str(labels[i]))\n",
    "    plt.show()\n",
    "\n",
    "# one_hot 编码，对标签进行 one_hot\n",
    "def one_hot(labels):\n",
    "    onehot_labels=np.zeros(shape=[len(labels),10])\n",
    "    for i in range(len(labels)):\n",
    "        index=labels[i]\n",
    "        onehot_labels[i][index]=1\n",
    "    return onehot_labels\n",
    " \n",
    "# 搭建 简单 网络 \n",
    "def mnist_net(input_shape):\n",
    "    '''\n",
    "    构建一个简单的全连接层网络模型：\n",
    "    输入层为28x28=784个输入节点\n",
    "    隐藏层120个节点\n",
    "    输出层10个节点\n",
    "    :param input_shape: 指定输入维度\n",
    "    :return:\n",
    "    '''\n",
    "    # 使用 顺序模型\n",
    "    model = keras.Sequential()\n",
    "    # 输入层  input_shape 输入的 大小\n",
    "    model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "    #隐含层  units 神经元的个数120，activation 激活函数 位 rule\n",
    "    model.add(keras.layers.Dense(units=120, activation=tf.nn.relu)) \n",
    "    #输出层 units 神经元的个数10（10个数字），activation 激活函数 位 softmax\n",
    "    model.add(keras.layers.Dense(units=10, activation=tf.nn.softmax))\n",
    "    return model\n",
    " \n",
    "# 搭建 CNN \n",
    "def mnist_cnn(input_shape):\n",
    "    '''\n",
    "    构建一个CNN网络模型\n",
    "    :param input_shape: 指定输入维度\n",
    "    :return:\n",
    "    '''\n",
    "    # 使用 顺序模型\n",
    "    model=keras.Sequential()\n",
    "    # 卷积层 ，kernel_size卷积核大小，activation ，激活函数 ，input_shape 输入大小\n",
    "    model.add(keras.layers.Conv2D(filters=32,kernel_size = 5,strides = (1,1),\n",
    "                                  padding = 'same',activation = tf.nn.relu,input_shape = input_shape))\n",
    "    # 池化层 pool_size 池化大小\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "     # 卷积层 \n",
    "    model.add(keras.layers.Conv2D(filters=64,kernel_size = 3,strides = (1,1),padding = 'same',activation = tf.nn.relu))\n",
    "    # 池化层\n",
    "    model.add(keras.layers.MaxPool2D(pool_size=(2,2), strides = (2,2), padding = 'valid'))\n",
    "    # 需要丢弃的输入比例。可以防止过拟合\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    # 展平一个张量\n",
    "    model.add(keras.layers.Flatten())\n",
    "    #全连接层\n",
    "    model.add(keras.layers.Dense(units=128,activation = tf.nn.relu))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    # 全连接层 输出\n",
    "    model.add(keras.layers.Dense(units=10,activation = tf.nn.softmax))\n",
    "    return model\n",
    "\n",
    "# 训练 模型\n",
    "def trian_model(train_images,train_labels,test_images,test_labels):\n",
    "    # re-scale to 0~1.0之间，把数据 压缩到 0-1 之间\n",
    "    train_images=train_images/255.0\n",
    "    test_images=test_images/255.0\n",
    "    # mnist数据转换为四维\n",
    "    train_images=np.expand_dims(train_images,axis = 3)\n",
    "    test_images=np.expand_dims(test_images,axis = 3)\n",
    "    print(\"train_images :{}\".format(train_images.shape))\n",
    "    print(\"test_images :{}\".format(test_images.shape))\n",
    "    # 标签 one—hot \n",
    "    train_labels=one_hot(train_labels)\n",
    "    test_labels=one_hot(test_labels)\n",
    " \n",
    "    # 建立模型\n",
    "    model = mnist_net(input_shape=(28,28,1)) # 简单 网络\n",
    "    #model=mnist_cnn(input_shape=(28,28,1))  # CNN 网络\n",
    "    #配置训练模型。optimizer优化器，loss损失函数\n",
    "    model.compile(optimizer=tf.optimizers.Adam(),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    # 开始 训练 模型 \n",
    "    model.fit(x=train_images,y=train_labels,epochs=5,validation_data=(test_images,test_labels),callbacks=[history])\n",
    "    \n",
    "    # 测试模式 返回 损失值和准确率。\n",
    "    test_loss,test_acc=model.evaluate(x=test_images,y=test_labels)\n",
    "    print(\"Test Accuracy %.2f\"%test_acc)\n",
    " \n",
    "    # 开始预测\n",
    "    cnt=0\n",
    "    predictions=model.predict(test_images)\n",
    "    for i in range(len(test_images)):\n",
    "        target=np.argmax(predictions[i])\n",
    "        label=np.argmax(test_labels[i])\n",
    "        if target==label:\n",
    "            cnt +=1\n",
    "    # 准确率\n",
    "    print(\"True prediction of total : %.2f\"%(cnt/len(test_images)))\n",
    "    # 保存 模型\n",
    "    model.save('mnist-model.h5')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = '/kaggle/input/dataet/mnist.npz'\n",
    "# 获取 数据 \n",
    "train_images, train_labels, test_images, test_labels=get_train_val(mnist_path)\n",
    "# 展示 数据 \n",
    "show_mnist(train_images, train_labels)\n",
    "# 训练 和 预测\n",
    "trian_model(train_images, train_labels, test_images, test_labels)\n",
    "# 绘制acc-loss曲线\n",
    "history.loss_plot('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "mnist_path = '/kaggle/input/dataet/mnist.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mnist_path):\n",
    "    \"\"\"Loads the MNIST dataset.\n",
    "    # Arguments\n",
    "        path: path where to cache the dataset locally\n",
    "            (relative to ~/.keras/datasets).\n",
    "    # Returns\n",
    "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
    "    \"\"\"\n",
    "    #path = get_file(path,\n",
    "    #                origin='https://s3.amazonaws.com/img-datasets/mnist.npz',\n",
    "    #                file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n",
    "    f = np.load(path)\n",
    "    x_train, y_train = f['x_train'], f['y_train']\n",
    "    x_test, y_test = f['x_test'], f['y_test']\n",
    "    f.close()\n",
    "    return (x_train, y_train), (x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
